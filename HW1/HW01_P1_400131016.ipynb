{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PART 1 : Q2_a"
      ],
      "metadata": {
        "id": "Fq4D5FPsqtZ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMo1MrAEql3L",
        "outputId": "ff7ba56a-d043-418a-864c-bd2ffe92737d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=cc33242fcd5e5f13ac894a8dbdaca51df1bc50280cd77d887c66ce49bde11fd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark import SparkConf, SparkContext"
      ],
      "metadata": {
        "id": "tw-JDJc5rXvH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the Spark context\n",
        "conf = SparkConf().setAppName(\"ChannelPromotions\").setMaster(\"local\")\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "\n",
        "# read in the data from the input file\n",
        "data = sc.textFile(\"data.txt\")\n",
        "\n",
        "# map the channel IDs to (ID, 1) pairs\n",
        "channel_counts = data.flatMap(lambda line: line.split()[1:]) \\\n",
        "                     .map(lambda channel_id: (channel_id, 1))\n",
        "\n",
        "# reduce the channel counts by summing the values for each key\n",
        "channel_totals = channel_counts.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# find the top 5 channels with the most link exchanges\n",
        "top_channels = channel_totals.takeOrdered(5, key=lambda x: -x[1])\n",
        "\n",
        "\n",
        "# print the results\n",
        "for channel, count in top_channels:\n",
        "    print(f\"Channel {channel} has been advertised {count} times.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1cZUtX3tO-C",
        "outputId": "4020c03b-5ba1-49a8-c544-f67bf0b6c65f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channel 859 has been advertised 1933 times.\n",
            "Channel 5306 has been advertised 1741 times.\n",
            "Channel 2664 has been advertised 1528 times.\n",
            "Channel 5716 has been advertised 1426 times.\n",
            "Channel 6306 has been advertised 1394 times.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PAR1 : Q2_b"
      ],
      "metadata": {
        "id": "Fi2K0WcUtURp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contribution(channel_id):\n",
        "\n",
        "  # specify the channel to find promotion count for\n",
        "  target_channel = channel_id\n",
        "  # filter the data to only include transactions for the target channel\n",
        "  target_data = data.filter(lambda line: target_channel in line.split()[1:])\n",
        "\n",
        "  # map the channel IDs to (ID, 1) pairs\n",
        "  channel_counts = target_data.flatMap(lambda line: line.split()[1:]) \\\n",
        "                              .map(lambda channel_id: (channel_id, 1))\n",
        "\n",
        "  # reduce the channel counts by summing the values for each key\n",
        "  channel_totals = channel_counts.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "  # find the total promotion count for the target channel\n",
        "  promotion_count = channel_totals.filter(lambda x: x[0] == target_channel).map(lambda x: x[1]).collect()\n",
        "\n",
        "  # print the result\n",
        "  print(f\"The exchange count for channel {target_channel} is {promotion_count[0]}.\")\n",
        "\n",
        "\n",
        "contribution('1748')\n",
        "contribution('5633')\n",
        "contribution('3469')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQC-aQA-tRoc",
        "outputId": "d5fcd900-2935-41f4-a497-4b051393afe3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The exchange count for channel 1748 is 130.\n",
            "The exchange count for channel 5633 is 30.\n",
            "The exchange count for channel 3469 is 119.\n"
          ]
        }
      ]
    }
  ]
}